# Summary

| Date   | Notes
| :----- | :-------------------------------
| 06/05 | Team meeting on week tasks. Re-read the two papers: Investigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation and Searching for Problematic Simulation Conditions
| 06/06 | Team meeting for updates on Twinmotion & UE work. Explored our datasets and how to navigate those.
| 06/07 | Finished fastai tutorial #2, created a custom UFO image classifier model and deployed it.
| 06/08 | Tried training our 2ndRunHighRes dataset with the 4 chosen CNN models from the research paper and compared their accuracies.
| 06/09 | Found that Resnet34 has the best performance and training time => deployed the model with resnet34 trained on 2ndRunHighRes dataset
| 06/10 | Started trying to rebuild Command+Image model


# Activities 

# 1. Created a UFO vs stars image classifier model to practice with FastAI. 

* Following FastAi documentation and tutorials, I created the confusion matrix and cleaner to clean my data and improve models' performance. After that, I deployed the model using Gradio + HuggingFace. Here is the link to the live model: https://huggingface.co/spaces/chauhavu/ufo-model.
<img width="500" alt="Screenshot 2023-06-12 at 1 19 13 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2c49520c-d585-4c44-bbfb-eff843de0406">
<img width="500" alt="Screenshot 2023-06-12 at 1 19 32 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2d7eaa15-3eb7-4309-8a34-5f59a9b64489">
<img width="500" alt="Screenshot 2023-06-12 at 1 20 18 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/25e17192-74d7-4947-9cc6-652fa6859854">

<img width="1270" alt="Screenshot 2023-06-12 at 1 31 47 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/f7aec069-81db-4460-a8eb-6c560842eaa9">

* Experimenting with current UE dataset: data/clark/data/2ndRunHighRes, OldenborgSmall-2022-07-11, Oldenborg-2022-07-01, and Office127-2022-07-06. 
   * I found that the Oldenborg-2022-07-01 dataset can be very helpful to test models since these are real pictures from oldenborg hallways.
   * Only the 2ndRunHighRes data includes images with the same size. Therefore, we can use ImageDataLoaders for these:
  ```dls_train_2 = ImageDataLoaders.from_name_func('/data/clark/data/2ndRunHighRes', get_image_files('/data/clark/data/2ndRunHighRes'), filename_to_class, valid_pct=VALID_PCT, items_tfms=Resize(128))```
   * The other three datasets are not similar in size. If we want to use it, have to use DataBlock to set the custom size.
  <img width="1035" alt="Screenshot 2023-06-12 at 1 23 25 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/c1beb780-164b-4b29-bd63-0fc6ec826b68">

# 2. Comparing the CNN models that FastAI Vision learner supports
 * I compared the 4 models chosen the in paper Investigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation including: resnet50, resnet34, alexnet, and resnet18. 
 * I trained all 4 models on the ```data/clark/data/2ndRunHighRes``` dataset, and here are the results:
 <img width="800" alt="Screenshot 2023-06-12 at 1 29 36 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/c36e14e5-efc4-4d06-9824-52a7e30723ee">
<img width="800" alt="Screenshot 2023-06-12 at 1 29 51 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/0b92a7ef-cfd7-490e-adff-ecd24542f9a6">
<img width="800" alt="Screenshot 2023-06-12 at 1 30 01 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/c8d9bc70-894e-4e9d-a0ba-2811a940cd5a">
<img width="800" alt="Screenshot 2023-06-12 at 1 30 17 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/05ff8502-1fb0-4936-ad34-5e6c4ab5e84e">

 * I found that Resnet34 has the highest accuracy and deployed the this model to HuggingFace. Here is the link to the live model: https://huggingface.co/spaces/chauhavu/oldenborg-model.  I also included the sample test cases from the Oldenborg-2022-07-01 dataset. 

* Continued with FastAI and reading its documentation. 
* Started trying out different models in the first paper to compare resnet50, resnet34, alexnet, and resnet18.
* Found that resnet34 is the fastest and most accurate => deploy the model trained using 2ndRunHighRes to hugging motion: https://huggingface.co/spaces/chauhavu/oldenborg-model. I included the test cases from the Oldenborg-2022-07-01 dataset
* Rebuild Command+Image (ongoing)

# Issues

* Need to resize all the images in a folder to the same size so that the dls can run. Or else it will give this error: 
<img width="993" alt="Screenshot 2023-06-10 at 5 14 56 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/703749cb-72e2-4196-a40f-18d286be43b4">
<img width="1028" alt="Screenshot 2023-06-10 at 5 15 48 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/15a5785f-7b42-4bd2-8dbe-61eb1f45db1a">

* How to view the pictures in a dataset models. The best way for me is just probably to put it into a datablock and then showbatch() because in a mac terminal there are no open good way to open it straight from shell commands.

* The dls.show_batch() method and dls.valid.show_batch() are similar in that they both display a batch of images from the dataset. However, there is a subtle difference between the two:
  * dls.show_batch(): This method displays a batch of images from the entire dataset, including both the training and validation sets. It randomly samples images from the combined dataset and displays them in a grid. It is useful for quickly getting an overview of the data and checking if the images are correctly loaded.
  * dls.valid.show_batch(): This method displays a batch of images specifically from the validation dataset. It only samples images from the validation set and displays them in a grid. It helps in examining the images that the model will be evaluated on during validation.
  
  * How to check the number of batch for training and then for validation:
<img width="760" alt="Screenshot 2023-06-10 at 5 24 00 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2664debd-530c-430c-9b3c-edb82406f410">

```num_valid_data = len(dls.valid.dataset)```

```num_train_data = len(dls.train.dataset)```

```num_valid_data```


* The problem with deployment on HuggingFace:
  * Make changes in the the app.py file in Visual Studio Code. Here is my code in ```app.py``` with the first model:
<img width="800" alt="Screenshot 2023-06-11 at 5 42 49 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/b17c1a27-a4fd-492c-be36-ac5d442cb96f">
  *  Use git.lfs
  *  Add requirement.txt with fastai and scikit-image

* Run on one window at a time because or else the memory will run out. 

# Todo (tmr)
* (done) Finish fastai tutorial #2, understand and use the confusion matrix, and deploy a simple model first.
* Explore 24 standard CNN networks and train each of those on 3 datasets (handmade, uniform, and wandering, and maybe other datasets like francisco did with MuddHallway), focus on the best 4 models - XResNeXt18, XResNeXt50, AlexNet, DenseNet121. (just train with 5% validation first, and then measure performance after ask about how to find 20 validation maps). 
* Deploy each of the four models above after the tutorial in fastai. (Francisco tried 3, maybe the 4th one is difficult because it is not supported in vision_learner => try to do the 4th one then).
* Try to contribute to the ARCS lab docs with my experiments with the standard CNN networks. (how to access datasets, how I tested,...)

# Questions:
* Question regarding validation and performance evaluation: 
  * 5% of the training data will be used for validation => Validation score
  * Each of these models will be evaluated on 20 validation maps, which were not seen during training => performance score. How can I find these 20 validation maps when I try to replicate the models?

# Goal:
* Christy and Liz need help with more complicated models that retain the previous states (like the hybrid ones, and RNNs) and do something with their ideas on adversarial and domain randomization. So, I need to be able to understand, and replicate their existing models, before asking on direction to do more and train more complicated models with the new dataset and their direction ideally in the fall.

# Research readings
* Read the first research paper and compare the pretrained models using jupyternotebook. Try to understand the methods used. 

  


