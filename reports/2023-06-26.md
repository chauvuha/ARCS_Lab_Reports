# Summary

| Date   | Notes
| :----- | :-------------------------------
| 06/19 | Team meeting on week tasks, demo walkthroughs with professor Clark on the ue5osc repository.
| 06/20 | Watched the W&B tutorial #1. Experiment with the package functionalities (reports, graphs,...) myself.
| 06/21 | Worked with the package from Francisco and Anjali, cloned the ue5osc repository.
| 06/22 | Tried to troubleshoot the problem with the screenshot command in the package. Experiment with Archicad. Continued the MLOps course
| 06/23 | Continue experimenting with Archicad. Started reading the research paper Liz put in Slack. Continued the MLOps course
| 06/24 | Continue experimenting with Archicad. Continued the MLOps course

# 1. Activities

# 1.1. Experiment with W&B functionalities:

* Several notes:
  * Weights & Biases were there to keep track of the problems that might arise when we train the models (whether it is parameters tuning or the training process, etc).
  * Extra exciting things that we can do include:
      * Visualize how parameters (training time, or a number of batches, epochs, etc,...) can affect the performance of the model.
      * Sweeps in W&B can help choose the best hyperparameters for the model. 

* I was experimenting to see if there is a correlation between the number of epochs and performance in terms of loss in the first graph. 
<img width="1134" alt="Screenshot 2023-06-24 at 1 55 56 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/627416ed-b655-4ff5-a096-00cf281f2251">
<img width="1129" alt="Screenshot 2023-06-24 at 1 56 10 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/205f7e8b-e7f5-4469-bb10-c1b871a617ec">

* And we can also create reports to share with others very quickly because the graphs have been imported automatically.
<img width="1028" alt="Screenshot 2023-06-24 at 1 56 40 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/3d1adcfb-ad40-4c63-bb9f-b30c16070216">

# 1.2. Work with the UE Mac package + Experiment with the commands: 
* I downloaded the game package from Francisco and cloned the ue5osc repository.
* At first, I ran into this problem:
  
<img width="792" alt="Screenshot 2023-06-24 at 2 13 31 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/a6c04427-9788-44c3-ac03-c2006d9e3921">

* I solved this by activating conda, and installing by running

```pip install pythonosc```
  
* After that, I was able to test every functions and commands, except the screenshot ones.
* Several notes:
  * ke * texture [0-2] [0-40]. 
    * 0: Floor
    * 1: Wall
    * 2: Ceiling
  * pythonosc is not installed: pip install pythonosc.

<img width="1284" alt="Screenshot 2023-06-24 at 1 58 20 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/e4e56b63-6153-4497-8315-f0ce5884789b">
<img width="1142" alt="Screenshot 2023-06-24 at 1 58 54 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/26c475df-55be-4616-b488-bd5e452fdd52">

# 1.3 Experimented with Archicad
* Several notes:
  * Needs a license to use free. I have requested an education license with my pomona email, however, I have not heard back from them. It can take up to 5 days to process.
  * Instead I tried with the free demo version. This version does not allow me to save or copy the project so I was only able to try several functionalities.
 * Cons:
  * Definitely more complicated than SH3D.
  * Needs time to get familiar.
  * The walls are harder to create
  * Have not found a way to put the map as a blueprint. 
* Pros:
 * More complicated but many more options for textures, angles, furniture,... 
 * Very easy to create slopes, or inclines, different levels, and footings, which are beneficial when we recreate the Oldy model. (pictures below)
 * Not laggy
<img width="724" alt="Screenshot 2023-06-24 at 2 07 02 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/08673124-f1b0-45a1-ba66-a463b7e196c0">
<img width="686" alt="Screenshot 2023-06-24 at 2 07 17 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2d0f660c-91c0-4f31-a02e-b43e5934c288">
<img width="689" alt="Screenshot 2023-06-24 at 2 07 32 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/a0c81b4d-4044-42bd-b131-534fb6d754d1">

# Issues:
* Screenshot is the only function that is not working both from the repository and game console (E or F). -> How to print out and troubleshoot?

# Plans:
* Continued working with Archicad if I can have a license because I think it has potential.
* Finish the MLOps course.
* Try to figure out why the screenshots don't work.

# Article Summary (continue):
* Gibson Env: Real-World Perception for Embodied Agents: https://arxiv.org/pdf/1808.10654.pdf (on-going)
* Problem: Static datasets, pre-recorded videos are limited, so we use virtual environments. But these environments are of limited use for perception due to oversimplification of the visual world due to using synthetic underlying databases and/or rendering pipeline deficiencies.
=> Gibson addresses some of such concerns by striving to target perception in real-world via using (1) real spaces as its base, (2) a custom neural view synthesizer, and (3) a baked-in adaption mechanism (Goggles). (4) a library that makes the agent in Gibson environment subject to constraints of space and physics (e.g. collision, gravity).

    (1) Using real spaces as its base: Gibson’s underlying database of spaces: 572 full buildings composed of 1447 floors. The database is collected from real indoor spaces using 3D scanning and reconstruction. Within each space: the 3D reconstruction, RGB images, depth, surface normal, and for a fraction of the spaces, semantic object annotations.
    
    (2) A custom neural view synthesizer. Recent methods: employed neural networks in a rendering pipeline, e.g. via an encoderdecoder like architecture that directly renders pixels, with depth, we can make use of geometric approaches to be more robust to large viewpoint changes and implausible deformations. Gibson is a combination of above in which we geometrically render a base image for the target view, but resort to a neural network to correct artifacts and fill in the dis-occluded areas, along with jointly training a backward function for mapping real images onto the synthesized one. 
    
    (3) Domain Adaptation and Transferring to Real-World: Recent methods are domain randomization or forming joint spaces. For Gibson, they have large amounts of paired data for target-source domains available to train forward and backward => form a joint space => bakedin mechanism, minimizing the need for additional and custom adaptation (Goggles)
    
    (4) They use the library PyPilot

* Gibson Virtual Environment’s goal: provides the agent with a stream of visual observation, based on virtualizing real spaces, from arbitrary viewpoints as if the agent had an on-board camera. They want to ensure that the agent being trained and tested in this stream of images from Gibson will produce the same result when this stream of images is replaced with a stream of images from a real camera.



