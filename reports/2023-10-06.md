# Activities 
* Meeting with team about choosing between Twinmotion/ UE. 
* Watched FastAI tutorial #2 and created a model to ... and link... 
* Experimenting with current UE dataset (show code)
* Experimenting with maybe MuddHallway dataset (show code)
* Continued with FastAI and reading its documentation. 
* Started trying out different models in the first paper to compare resnet50, resnet34,... (show code)
* Rebuild Command+Image (ongoing)

# Issues
* The path to the datasets + how to test it easily (Francisco help)

* Need to resize all the images in a folder to the same size so that the dls can run. Or else it will give this error: 
<img width="993" alt="Screenshot 2023-06-10 at 5 14 56 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/703749cb-72e2-4196-a40f-18d286be43b4">
<img width="1028" alt="Screenshot 2023-06-10 at 5 15 48 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/15a5785f-7b42-4bd2-8dbe-61eb1f45db1a">

* How to view the pictures in a dataset models. The best way for me is just probably to put it into a datablock and then showbatch() because in a mac terminal there are no open good way to open it straight from shell commands.

* The dls.show_batch() method and dls.valid.show_batch() are similar in that they both display a batch of images from the dataset. However, there is a subtle difference between the two:
  * dls.show_batch(): This method displays a batch of images from the entire dataset, including both the training and validation sets. It randomly samples images from the combined dataset and displays them in a grid. It is useful for quickly getting an overview of the data and checking if the images are correctly loaded.
  * dls.valid.show_batch(): This method displays a batch of images specifically from the validation dataset. It only samples images from the validation set and displays them in a grid. It helps in examining the images that the model will be evaluated on during validation.
  * How to check the number of batch for training and then for validation:
<img width="760" alt="Screenshot 2023-06-10 at 5 24 00 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2664debd-530c-430c-9b3c-edb82406f410">

* Do not need to pip install fastbook, just need to run the four followings:

```from fastai import *```

```from fastbook import *```

```from fastai.vision.all import *```

```from fastai.callback.progress import CSVLogger```

# Todo (tmr)
* Finish fastai tutorial #2, understand and use the confusion matrix, and deploy a simple model first.
* Explore 24 standard CNN networks and train each of those on 3 datasets (handmade, uniform, and wandering, and maybe other datasets like francisco did with MuddHallway), focus on the best 4 models - XResNeXt18, XResNeXt50, AlexNet, DenseNet121. (just train with 5% validation first, and then measure performance after ask about how to find 20 validation maps). 
* Deploy each of the four models above after the tutorial in fastai. (Francisco tried 3, maybe the 4th one is difficult because it is not supported in vision_learner => try to do the 4th one then).

# Questions:
* Question regarding validation and performance evaluation: 
  * 5% of the training data will be used for validation => Validation score
  * Each of these models will be evaluated on 20 validation maps, which were not seen during training => performance score. How can I find these 20 validation maps when I try to replicate the models?

# Goal:
* Christy and Liz need help with more complicated models that retain the previous states (like the hybrid ones, and RNNs) and do something with their ideas on adversarial and domain randomization. So, I need to be able to understand, and replicate their existing models, before asking on direction to do more and train more complicated models with the new dataset and their direction ideally in the fall.

# Research readings
* Read the first research paper and compare the pretrained models using jupyternotebook. Try to understand the methods used. 

  


