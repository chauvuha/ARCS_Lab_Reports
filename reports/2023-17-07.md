# Summary

| Date   | Notes
| :----- | :-------------------------------
| 07/10 | Team meeting with Anjali, Francisco, and Prof Clark on last week's task and ue5osc code. Made changes to save_image() function to save at a specific path.
| 07/11 | Pulled the new code and new packaged MacOS game. Removed the path arguments in the main() function. Pushed the new changes.
| 07/12 | Started reading iGibson 2.0. Continued MLOps and followed along the code.
| 07/13 | Continued reading iGibson 2.0. Self-research on Gibson previous versions and future plans (Gibson v1, iGibson 1.0). Finished MLOps.
| 07/14 | Finished reading iGibson 2.0. 

# Activities:
1. Fixed the ue5osc code:
* Found that the path is not saving correctly. Made some changes with team for it to work with both Windows and MacOS
* Removed the path argument in init and the main() function:

~ python
def main():
    """Argument Parser that verifies that the image path is getting passed in and
    optional ability to set ip and ports."""
    parser = ArgumentParser()
    parser.add_argument("--ip", type=str, default="127.0.0.1", help="IP Address")
    parser.add_argument("--client_port", type=int, default=7447, help="Client Port")
    parser.add_argument("--server_port", type=int, default=7001, help="Server Port")
    parser.add_argument("--resolution", type=list, help="Set resolution of images.")
    args = parser.parse_args()

    with ue5osc.Communicator(
        args.ip, args.client_port, args.server_port, 
    ) as osc_communicator:
        print(osc_communicator.get_location()[0])
        sleep(1)
        print(osc_communicator.get_rotation())
        sleep(1)
        print(osc_communicator.get_project_name())
        sleep(1)
~

* The path is now correctly saved:
<img width="945" alt="Screenshot 2023-07-16 at 11 30 08 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/3bf1f1a0-356c-40e0-8f1d-40d2faa1daf8">
<img width="586" alt="Screenshot 2023-07-16 at 11 30 26 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/26676218-6697-4b17-bb5f-2286587968c2">

2. Read the iGibson 2.0 research paper:

* 1 to 5-sentence summary of the article:
  
  In this paper, the research team is introducing iGibson 2.0, an interactive simulation environment with features that support creating environments with more complicated household task in a scalable way. There are three main contributions: 
  First, it goes beyond just simulating motion and physical contact by keeping track of 5 object properties (temperature, wetness levels, cleanliness levels, toggled, and sliced states) and logical states (raw, cooked). This is very helpful for creating household task like cooking, or baking, or washing dishes.
  Second, it is scalable because it has a generative system that allows users to create the sample of the initial scenes based on pre-defined semantic rules automatically without having to manually put objects into locations, which saves significant time. For example, we can just put onTopof(apple, table) and the apple will be placed on top of the table in randomized position. They need to check the validity of these sample poses by techniques like raycasting to make sure it doesn't fall.
  Third, they also have VR interface to collect human demonstrations. The results show the potential of VR in collecting datasets for imitation learning for humanoid robots.

  * Finally, they did experiments to see the effect of the simulation using reinforcement learning and imitation learning. They collected 30 demonstrations with VR and used those for imitation learning with humanoid robots and Fetch robots. 

    * They created a set of functions that map these extended object states to logical states for single objects and pairs of objects; then based on these logical states,
  
* How does this paper relate to our lab’s work?
    We are also trying to create our simulation environment to develop and train robot learning approaches. Our simulation right now only covers motion & physical contact, and has not been able to cover other factors like object states, etc,...
    The paper focused on household tasks so they try to create a densely-populated environment, we focus on path mainly.

* What techniques should we apply to our work?
    * The idea of keeping track of the object's state. This is helpful because we are trying to solve the fundamental question of closing the sim2real gap:
        * In terms of the Oldenborg model: Doors that need to be toggled to open (Opened/Closed state), the light in the hallways are on/off and what should robots do in both situations? (On/Off state), are light sources coming from windows that might heat up things?
    * Apply VR to Unreal Engine: https://www.youtube.com/watch?v=DiGh6MxDFds&ab_channel=VirtusLearningHub 

* What could the authors have done better?
    * Suggestions:
        - Proximity-based heat sources/sinks: In addition to being inside, there could be cases where objects experience temperature changes based on the distance to source/sink (where do they put the distance in the formula?)
        - Dry state. Because objects can dry over time.
        - Slicing now only splits the object into half but VR is expensive to implement.

    * The paper:
        - A table to illustrate the differences between iGibson 2.0 and prior work? (Gibson 1, iGibson 1.0)
        - Should summarize the current limitations, and include open questions for future research that might want to extend this research. (These are included in the website).
        - I feel like the paper would be stronger to explore the benefits and drawbacks of learning-based agents further. Is partial observability (latent) an issue and why did they choose to assume that (costly? In future work?) ie. Are there household (or just general) cases where the agent would be better off keeping track of the history of previous observations/states?

* Questions:

In the paper, they assume that “the extended properties are latent: agents are not able to observe them directly”.
Is this partial observability (latent) an issue? ie. Are there situations where the agent needs to keep track of the history of previous observations?
Would it be better to do that? Like how Liz and Christy found that when we kept track of the previous state, the training performance significantly improved?

* Other notes:

Compare Gibson 1, iGibson 0.1, and iGibson 0.2

* Gibson 1: 
    * the precursor of iGibson.
    * Although Gibson incorporates PyBullet as its physics engine for simulating robot navigation, each scene is one single fully rigid object (static mesh).
    * Thus, it does not allow agents to interact with the scenes other than collisions with the mesh, restricting its use to only navigation.
    * A similar environment is Habitat.

* iGibson 0.1: 
    * Interactive versions of fifteen 3D reconstructed scenes included in the Gibson v1 dataset.
    * The object models are curated from open-source datasets: ShapeNet, PartNet Mobility, and SketchFab.
    * Improve the object visual quality by annotating different parts of the models with photorealistic materials. Additionally, provide compatibility with CubiCasa5K and 3D-Front repositories of leading to additional more than 12000 interactive home scenes. 

* iGibson 0.2: 
    * Added object states inside environment. 
    * Generate sample initial environment automatically based on predefined semantic rules and predicates. 
    * VR.

3. Notes on MLOps:

* Split data
  
```processed_data_at_new = wandb.Artifact(params.PROCESSED_DATA_AT, type="split_data") ```

* Then we add to the artifact:

```processed_data_at_new.add(join_table, "eda_table_data_split")```

* Save raw data
```raw_data_at = wandb.Artifact(params.RAW_DATA_AT, type="raw_data")```

* Training: After run baseline -> refactor -> we would want to export everything to a train.py file. and then use parse.arges to run it in command line (the command: python train.py –batch_size 16). If dont have/want command line, we can run with a file like this: https://github.com/anthonyjclark/raycasting-simulation/blob/master/Experiments/RunCmdClassification.py 

# Plans:
* I plan to try working with wandb for our image classification problem. (the tutorial is about an image segmentation problem)
* Start data collection and train with Command+Image. About our plans:
    * How can we run it from beginning to end, which command?  (automatic screenshots done)
    * How many dataset of screenshots are we going to take? Or we collect one, and augment it?
