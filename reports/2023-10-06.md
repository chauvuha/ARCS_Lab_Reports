# Summary

| Date   | Notes
| :----- | :-------------------------------
| 06/05 | Team meeting on week tasks. Re-read the two papers: Investigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation and Searching for Problematic Simulation Conditions
| 06/06 | Team meeting for updates on Twinmotion & UE work. Explored our datasets and how to navigate those.
| 06/07 | Finished fastai tutorial #2, created a custom UFO image classifier model and deployed it.
| 06/08 | Tried training our 2ndRunHighRes dataset with the 4 chosen CNN models from the research paper and compared their accuracies.
| 06/09 | Found that Resnet34 has the best performance and training time => deployed the model with resnet34 trained on 2ndRunHighRes dataset
| 06/10 | Started trying to rebuild Command+Image model


# Activities 

# 1. Created a UFO vs stars image classifier model to practice with FastAI. 

* Following FastAi documentation and tutorials, I created the confusion matrix and cleaner to clean my data and improve models' performance. After that, I deployed the model using Gradio + HuggingFace. Here is the link to the live model: https://huggingface.co/spaces/chauhavu/ufo-model.
* 
<img width="500" alt="Screenshot 2023-06-12 at 1 19 13 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2c49520c-d585-4c44-bbfb-eff843de0406">

<img width="500" alt="Screenshot 2023-06-12 at 1 19 32 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2d7eaa15-3eb7-4309-8a34-5f59a9b64489">

<img width="500" alt="Screenshot 2023-06-12 at 1 20 18 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/25e17192-74d7-4947-9cc6-652fa6859854">

<img width="1270" alt="Screenshot 2023-06-12 at 1 31 47 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/f7aec069-81db-4460-a8eb-6c560842eaa9">

* Experimenting with current UE dataset(data/clark/data/2ndRunHighRes, OldenborgSmall-2022-07-11, Oldenborg-2022-07-01, and Office127-2022-07-0): 
   * I found that the Oldenborg-2022-07-01 dataset can be very helpful to test models since these are real pictures from oldenborg hallways.
   * Only the 2ndRunHighRes data includes images with the same size. Therefore, we can use ImageDataLoaders for these:
  ```dls_train_2 = ImageDataLoaders.from_name_func('/data/clark/data/2ndRunHighRes', get_image_files('/data/clark/data/2ndRunHighRes'), filename_to_class, valid_pct=VALID_PCT, items_tfms=Resize(128))```
   * The other three datasets are not similar in size. If we want to use it, we have to use DataBlock to set the custom size.
  
  <img width="1035" alt="Screenshot 2023-06-12 at 1 23 25 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/c1beb780-164b-4b29-bd63-0fc6ec826b68">

# 2. Compared the CNN models from the research paper that FastAI Vision learner supports:
 * I compared the 4 models chosen the in paper Investigating Neural Network Architectures, Techniques, and Datasets for Autonomous Navigation in Simulation including: resnet50, resnet34, alexnet, and resnet18. 
 * I trained all 4 models on the ```data/clark/data/2ndRunHighRes``` dataset, and here are the results:
 <img width="800" alt="Screenshot 2023-06-12 at 1 29 36 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/c36e14e5-efc4-4d06-9824-52a7e30723ee">
 
<img width="800" alt="Screenshot 2023-06-12 at 1 29 51 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/0b92a7ef-cfd7-490e-adff-ecd24542f9a6">

<img width="800" alt="Screenshot 2023-06-12 at 1 30 01 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/c8d9bc70-894e-4e9d-a0ba-2811a940cd5a">

<img width="800" alt="Screenshot 2023-06-12 at 1 30 17 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/05ff8502-1fb0-4936-ad34-5e6c4ab5e84e">

 * I found that Resnet34 has the highest accuracy and deployed the this model to HuggingFace. Here is the link to the live model: https://huggingface.co/spaces/chauhavu/oldenborg-model.  I also included the sample test cases from the Oldenborg-2022-07-01 dataset. 

# 3. Rebuilding Command+Image (ongoing)
* I have several questions to investigate:
    * I am already trying to replicate the stack, panel, and hybrid model. But for the custom models (paneled and stacked) that were created based on the 4 chosen CNN networks, how exactly can I stack or panel (integrate the previous action or input into the model architecture)? I need to spend more time to understand the code and architecture of these models. Do you think it is better to try the stack and panel first?...
    * 

# Issues

* This error indicates that all the images need to be of the same size so that the dls can run: 
* 
<img width="993" alt="Screenshot 2023-06-10 at 5 14 56 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/703749cb-72e2-4196-a40f-18d286be43b4">

<img width="1028" alt="Screenshot 2023-06-10 at 5 15 48 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/15a5785f-7b42-4bd2-8dbe-61eb1f45db1a">

  => Solution: Use Datablock's Resize.

* How to view the pictures in a dataset models in macbook. Currently, the best way for me is just probably to put it into a datablock and then showbatch() because I had not found a good way to open it straight from shell commands.

  * How to check the number of batch for training and then for validation:
  * 
<img width="760" alt="Screenshot 2023-06-10 at 5 24 00 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2664debd-530c-430c-9b3c-edb82406f410">

```num_valid_data = len(dls.valid.dataset)```

```num_train_data = len(dls.train.dataset)```


* I ran into several problems with git and hugging face when trying to deploy my models that are not covered in the tutorial. Here is the right steps to deploy.
    * Download the trained model in jupyter notebook as a .pkl file.
    * Created a HuggingFace space and clone it. Then open in VSCode.
    * Move the downloaded .pkl file into the open directory in VSCode.
    * Add a ```app.py``` file in the open directory in VSCode. Here is my full code in ```app.py``` for it to work:
    * 
  <img width="800" alt="Screenshot 2023-06-11 at 5 42 49 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/b17c1a27-a4fd-492c-be36-ac5d442cb96f">
  
    *  Add requirement.txt file in the open directory in VSCode. The file includes two lines: fastai and scikit-image:
  <img width="500" alt="Screenshot 2023-06-12 at 1 48 26 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/b4c652c1-af50-4462-9149-2be6725fa49f">
  
    * If you want to add examples to the deployed model, you can add some pictures to the local directory (Here I added 'star.webp', 'ufo.webp', and 'star.webp').
    *  Run these commands in the terminal (These will help install lfs, which will take care of the .pkl file of our model)

```git lfs install```

```git lfs track "*.pkl"```

```git add .gitattributes```

``` git add .```

```git commit -am "let's deploy to huggingface spaces"```

``` git push```

    * The final file structure of my VSCode look like this:
    
<img width="169" alt="Screenshot 2023-06-12 at 1 52 52 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/a77bb64f-c27c-4e2e-85e6-3553daa5a887">


* Run on one window at a time because or else the memory will run out. 

# Research readings
* Read the first research paper and compare the pretrained models using jupyternotebook. 

  


