# Summary

| Date   | Notes
| :----- | :-------------------------------
| 07/10 | Team meeting with Anjali, Francisco, and Prof Clark on last week's task and ue5osc code. Made changes to save_image() function to save at a specific path.
| 07/11 | Pulled the new code and new packaged MacOS game. Removed the path arguments in the main() function. Pushed the new changes.
| 07/12 | Started reading iGibson 2.0
| 07/13 | Continued reading iGibson 2.0. Self-research on Gibson previous versions and future plans (Gibson v1, iGibson 1.0). Finished MLOps.
| 07/14 | Finished reading iGibson 2.0. 

# Activities:
1. Fixed the ue5osc code:
* Found that the path is not saving correctly. Made some changes with team for it to work with both Windows and MacOS
* Removed the path argument in init and the main() function:

~ python
def main():
    """Argument Parser that verifies that the image path is getting passed in and
    optional ability to set ip and ports."""
    parser = ArgumentParser()
    parser.add_argument("--ip", type=str, default="127.0.0.1", help="IP Address")
    parser.add_argument("--client_port", type=int, default=7447, help="Client Port")
    parser.add_argument("--server_port", type=int, default=7001, help="Server Port")
    parser.add_argument("--resolution", type=list, help="Set resolution of images.")
    args = parser.parse_args()

    with ue5osc.Communicator(
        args.ip, args.client_port, args.server_port, 
    ) as osc_communicator:
        print(osc_communicator.get_location()[0])
        sleep(1)
        print(osc_communicator.get_rotation())
        sleep(1)
        print(osc_communicator.get_project_name())
        sleep(1)
~

* The path is now correctly saved:
<img width="945" alt="Screenshot 2023-07-16 at 11 30 08 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/3bf1f1a0-356c-40e0-8f1d-40d2faa1daf8">
<img width="586" alt="Screenshot 2023-07-16 at 11 30 26 AM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/26676218-6697-4b17-bb5f-2286587968c2">

2. Read the iGibson 2.0 research paper:

* 1 to 5-sentence summary of the article:
  In this paper, the research team is introducing iGibson 2.0, a simulation environment with novel features that support the simulation of a more complicated and diverse set of household tasks.
  First, it goes beyond just simulating motion and physical contact like previous work on simulation environments by keeping track of 5 object states (temperature, wetness levels, cleanliness levels, toggled, and sliced states).
  Second, they created a set of functions that map these extended object states to logical states for single objects and pairs of objects; then based on these logical states, Gibson has a generative system that allows it to create the sample of the initial scenes automatically without having to manually put objects into locations, which saves significant time while creating the simulation.
  Third, iGibson 2.0 includes a VR interface to immerse humans in its scenes to collect data (demonstrations). Finally, they did experiments to see the effect of the simulation using reinforcement learning and imitation learning. They collected 30 demonstrations with VR and used those for imitation learning with humanoid robots and Fetch robots.
  The results show the very potential of VR in collecting datasets for imitation learning, and a novel simulation method to study complex household tasks in a scalable way. 

* How does this paper relate to our labâ€™s work?
    We are also trying to create our simulation environment to develop and train robot learning approaches. Our simulation right now only covers motion & physical contact, and has not been able to cover other factors like object states, etc,...
    The paper focused on household tasks so they try to create a densely-populated environment, we focus on path mainly.

* What techniques should we apply to our work?
    * The idea of keeping track of the object's state. This is helpful because we are trying to solve the fundamental question of closing the sim2real gap:
        * In terms of the Oldenborg model: Doors that need to be toggled to open (Opened/Closed state), the light in the hallways are on/off and what should robots do in both situations? (On/Off state), are light sources coming from windows that might heat up things?
    * Apply VR to Unreal Engine: https://www.youtube.com/watch?v=DiGh6MxDFds&ab_channel=VirtusLearningHub 
