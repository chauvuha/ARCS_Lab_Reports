# Summary

| Date   | Notes
| :----- | :-------------------------------
| 06/28 | Weekly meeting. Finished MLOps #1. 
| 06/29 | Decided that will move on with SH3D. Worked on added stairs and ramps.
| 06/30 | Continued building the Oldenborg model.
| 06/31 | Joined meeting with Prof Clark, Francisco, and Anjali on UE Coding. Added the rails, door columns on Daisy's model. Started creating the table dimensions.
| 07/01 | Added missing stairs to the Oldenborf model. Started SWOT

# Activites
# 1. MLOps lesson:
Lesson 1: Introduce a business problem and develop a simple baseline. 
Lesson 2: Optimize the solution: introduce hyperparameter tuning (how to work in a team)
Lesson 3: Discuss model evaluation,

-> learn principle machine learning workflows. (ideas, artifacts, experiments, tables, sweeps, reports,...). Focusing on model development and evaluation, skip data collection

* W&B Tables
  * Visualize and analyze model predictions
  * Analyze and inspect tabular data including rich media such as images or videos.
  * Centralize exploratory data analysis
  * Do at the beginning or while training the data, can also use to compare the performance of experiments

* W&B Artifacts:
  * Lightweight dataset and model versioning with deduplication
  * track versions of your datasets and models.
  * Save every step of your pipeline and for reproduction. 

* W&B Experiments Management System
  * A system of record
  * Visualize and compare experiments, metrics,...

* W&B Reports: 
  * Collaborative dashboards
  * Add tables, visualizations + descriptions + live comments
  * Latex zip or pdf

# 2. The screenshot from the game package from Franciso:
* Issue resolved: the screenshots can be found at: ```~/Library/Application Support/Epic/ARCSAssets/Saved/Screenshots/Mac/ ```
  
# 3. Continued building Oldenborg with Sweet Home 3D 
* Added stairs & ramps created by Daisy using the position of these from the map by Anjali
* Imported railings and door columns
* Created a table to scale down the dimensions from the actual measurements
<img width="790" alt="Screenshot 2023-06-30 at 1 29 46 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/c8881fae-d260-42c2-9245-a664accfd48a">
<img width="908" alt="Screenshot 2023-06-30 at 2 21 08 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/9c89617d-8279-4ad3-b5a2-fdcc9055881a">
<img width="909" alt="Screenshot 2023-06-30 at 3 02 53 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/b41050bd-bc33-4a45-a5dd-52af542ceeef">

# Article Summary (continue):
* Gibson Env: Real-World Perception for Embodied Agents: https://arxiv.org/pdf/1808.10654.pdf (on-going)
* Problem: Static datasets, pre-recorded videos are limited, so we use virtual environments. But these environments are of limited use for perception due to oversimplification of the visual world due to using synthetic underlying databases and/or rendering pipeline deficiencies.
=> Gibson addresses some of such concerns by striving to target perception in real-world via using (1) real spaces as its base, (2) a custom neural view synthesizer, and (3) a baked-in adaption mechanism (Goggles). (4) a library that makes the agent in Gibson environment subject to constraints of space and physics (e.g. collision, gravity).

    (1) Using real spaces as its base: Gibson’s underlying database of spaces: 572 full buildings composed of 1447 floors. The database is collected from real indoor spaces using 3D scanning and reconstruction. Within each space: the 3D reconstruction, RGB images, depth, surface normal, and for a fraction of the spaces, semantic object annotations.
    
    (2) A custom neural view synthesizer. Recent methods: employed neural networks in a rendering pipeline, e.g. via an encoderdecoder like architecture that directly renders pixels, with depth, we can make use of geometric approaches to be more robust to large viewpoint changes and implausible deformations. Gibson is a combination of above in which we geometrically render a base image for the target view, but resort to a neural network to correct artifacts and fill in the dis-occluded areas, along with jointly training a backward function for mapping real images onto the synthesized one. 
    
    (3) Domain Adaptation and Transferring to Real-World: Recent methods are domain randomization or forming joint spaces. For Gibson, they have large amounts of paired data for target-source domains available to train forward and backward => form a joint space => bakedin mechanism, minimizing the need for additional and custom adaptation (Goggles)
    
    (4) They use the library PyPilot

* Gibson Virtual Environment’s goal: provides the agent with a stream of visual observation, based on virtualizing real spaces, from arbitrary viewpoints as if the agent had an on-board camera. They want to ensure that the agent being trained and tested in this stream of images from Gibson will produce the same result when this stream of images is replaced with a stream of images from a real camera.

# Plans:
* Continued building Oldenborg in SH3D



