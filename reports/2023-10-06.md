# Activities 
* Meeting with team about choosing between Twinmotion/ UE. 
* Watched FastAI tutorial #2 and created a model to ... and link... 
* Experimenting with current UE dataset (show code)
* Experimenting with maybe MuddHallway dataset (show code)
* Continued with FastAI and reading its documentation. 
* Started trying out different models in the first paper to compare resnet50, resnet34,... (show code)
* Rebuild Command+Image (ongoing)

# Issues
* The path to the datasets + how to test it easily (Francisco help)

* Need to resize all the images in a folder to the same size so that the dls can run. Or else it will give this error: 
<img width="993" alt="Screenshot 2023-06-10 at 5 14 56 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/703749cb-72e2-4196-a40f-18d286be43b4">
<img width="1028" alt="Screenshot 2023-06-10 at 5 15 48 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/15a5785f-7b42-4bd2-8dbe-61eb1f45db1a">

* How to view the pictures in a dataset models. The best way for me is just probably to put it into a datablock and then showbatch() because in a mac terminal there are no open good way to open it straight from shell commands.

* The dls.show_batch() method and dls.valid.show_batch() are similar in that they both display a batch of images from the dataset. However, there is a subtle difference between the two:
  * dls.show_batch(): This method displays a batch of images from the entire dataset, including both the training and validation sets. It randomly samples images from the combined dataset and displays them in a grid. It is useful for quickly getting an overview of the data and checking if the images are correctly loaded.
  * dls.valid.show_batch(): This method displays a batch of images specifically from the validation dataset. It only samples images from the validation set and displays them in a grid. It helps in examining the images that the model will be evaluated on during validation.
  * How to check the number of batch for training and then for validation:
<img width="760" alt="Screenshot 2023-06-10 at 5 24 00 PM" src="https://github.com/chauvuha/ARCS_Lab_Reports/assets/79251745/2664debd-530c-430c-9b3c-edb82406f410">

# Todo (tmr)
* Finish fastai tutorial #2, understand and use the confusion matrix, and deploy a simple model first.
* Explore 24 standard CNN networks and train each of those on 3 datasets (handmade, uniform, and wandering, and maybe other datasets like francisco did with MuddHallway), focus on the best 4 models - XResNeXt18, XResNeXt50, AlexNet, DenseNet121. (just train with 5% validation first, and then measure performance after ask about how to find 20 validation maps). 
* Deploy each of the four models above after the tutorial in fastai. (Francisco tried 3, maybe the 4th one is difficult because it is not supported in vision_learner => try to do the 4th one then).

# Research readings
* Read the first research paper and compare the pretrained models using jupyternotebook. Try to understand the methods used. 
* Question regarding validation and performance evaluation: 
  * 5% of the training data will be used for validation => Validation score
  * Each of these models will be evaluated on 20 validation maps, which were not seen during training => performance score. How can I find these 20 validation maps when I try to replicate the models?
  


